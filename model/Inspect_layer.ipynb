{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MyPersonalTrainerr/Deep-Learning/blob/main/model/Inspect_layer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZSWvD_qIVv_n",
        "outputId": "156c1904-b123-4aa2-fedc-f5dc0c5375ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'mediapipe'...\n",
            "remote: Enumerating objects: 12113, done.\u001b[K\n",
            "remote: Counting objects: 100% (191/191), done.\u001b[K\n",
            "remote: Compressing objects: 100% (166/166), done.\u001b[K\n",
            "remote: Total 12113 (delta 40), reused 52 (delta 19), pack-reused 11922\u001b[K\n",
            "Receiving objects: 100% (12113/12113), 529.47 MiB | 11.88 MiB/s, done.\n",
            "Resolving deltas: 100% (8470/8470), done.\n",
            "Checking out files: 100% (2402/2402), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/google/mediapipe.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IEYxRSgUWZ9j",
        "outputId": "9b64296b-aa94-4acf-a73d-306d338a2ff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/mediapipe\n"
          ]
        }
      ],
      "source": [
        "%cd mediapipe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dk7w6gNtVx9I",
        "outputId": "0d47d4fc-f629-4a69-d016-a67d45b21863"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[{'name': 'input_1', 'index': 0, 'shape': array([  1, 256, 256,   3], dtype=int32), 'shape_signature': array([  1, 256, 256,   3], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}]\n",
            "=========================\n",
            "{'name': 'Identity', 'index': 310, 'shape': array([  1, 195], dtype=int32), 'shape_signature': array([  1, 195], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "{'name': 'Identity_1', 'index': 315, 'shape': array([1, 1], dtype=int32), 'shape_signature': array([1, 1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "{'name': 'Identity_2', 'index': 282, 'shape': array([  1, 256, 256,   1], dtype=int32), 'shape_signature': array([  1, 256, 256,   1], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "{'name': 'Identity_3', 'index': 283, 'shape': array([ 1, 64, 64, 39], dtype=int32), 'shape_signature': array([ 1, 64, 64, 39], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "{'name': 'Identity_4', 'index': 312, 'shape': array([  1, 117], dtype=int32), 'shape_signature': array([  1, 117], dtype=int32), 'dtype': <class 'numpy.float32'>, 'quantization': (0.0, 0), 'quantization_parameters': {'scales': array([], dtype=float32), 'zero_points': array([], dtype=int32), 'quantized_dimension': 0}, 'sparsity_parameters': {}}\n",
            "=========================\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "import timeit\n",
        "\n",
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=\"mediapipe/modules/pose_landmark/pose_landmark_lite.tflite\")\n",
        "# Get input and output tensors.\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# input details\n",
        "print(input_details)\n",
        "print(\"=========================\")\n",
        "\n",
        "# output details\n",
        "#  39*5, heatmap, presence, segmentation, visibility, 39*3\n",
        "for output in output_details:\n",
        "  print(output)\n",
        "print(\"=========================\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for image\n",
        "# check the type of the input tensor\n",
        "floating_model = input_details[0]['dtype'] == np.float32\n",
        "print(f'is floating model: {floating_model}')\n",
        "\n",
        "# NxHxWxC, H:1, W:2\n",
        "height = input_details[0]['shape'][1]\n",
        "width = input_details[0]['shape'][2]\n",
        "\n",
        "#                         image processing\n",
        "\n",
        "\n",
        "image = cv2.imread(\"football.jpg\")\n",
        "plt.imshow(image[:,:,::-1])\n",
        "image_cropped = image[:image.shape[0], :image.shape[0], ::-1] # crop to avoid letterboxing step\n",
        "plt.imshow(image_cropped)\n",
        "\n",
        "img = cv2.resize(image_cropped, (256, 256))[np.newaxis, :, :, :]\n",
        "img = (np.float32(img) - 0.0) / 255.0  # normalization (specified in tflite_converter_calculator, not in model card)\n",
        "plt.imshow(img.squeeze())\n",
        "\n",
        "#                           preciction\n",
        "\n",
        "interpreter.set_tensor(input_details[0]['index'], img)\n",
        "interpreter.invoke()\n",
        "output_face_landmarks1 = interpreter.get_tensor(output_details[0]['index'])\n",
        "output_face_landmarks2 = interpreter.get_tensor(output_details[1]['index'])\n",
        "output_face_landmarks3 = interpreter.get_tensor(output_details[2]['index'])\n",
        "output_face_landmarks4 = interpreter.get_tensor(output_details[3]['index'])\n",
        "output_face_landmarks5 = interpreter.get_tensor(output_details[4]['index'])\n",
        "\n",
        "\n",
        "#print(output_face_landmarks1.squeeze().shape)\n",
        "output_face_landmarks = output_face_landmarks1.reshape(39,5)\n",
        "#plt.imshow(image_cropped)\n",
        "output_face_landmarks = output_face_landmarks[:33,:]\n",
        "#print(output_face_landmarks)\n",
        "face_landmark_x = output_face_landmarks[:, 0:1]\n",
        "face_landmark_y = output_face_landmarks[:, 1:2]\n",
        "#plt.plot(face_landmark_x/256*image_cropped.shape[0], (face_landmark_y/256)*image_cropped.shape[1], '.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for video\n",
        "vid = cv2.VideoCapture(\"Basketball jump shot.mp4\")\n",
        "while(True):\n",
        "    try :\n",
        "        # Capture the video frame\n",
        "        # by frame\n",
        "        ret, image_cropped = vid.read()\n",
        "        image_cropped = cv2.cvtColor(image_cropped, cv2.COLOR_BGR2RGB)\n",
        "        #image_cropped = image[:image.shape[0], :image.shape[0], ::-1]\n",
        "        img = cv2.resize(image_cropped, (256, 256))[np.newaxis, :, :, :]\n",
        "        img = (np.float32(img) - 0.0) / 255.0\n",
        "        image_cropped = cv2.cvtColor(image_cropped, cv2.COLOR_RGB2BGR)\n",
        "        interpreter.set_tensor(input_details[0]['index'], img)\n",
        "        interpreter.invoke()\n",
        "        output_face_landmarks = interpreter.get_tensor(output_details[0]['index'])\n",
        "        #mask_0 = interpreter.get_tensor(output_details[0]['index']).reshape(39,5)\n",
        "        mask = interpreter.get_tensor(output_details[1]['index'])\n",
        "        mask_2 = interpreter.get_tensor(output_details[4]['index'])\n",
        "        mask_2 =mask_2.reshape(39,3)        \n",
        "        #print(mask)\n",
        "        #print(output_face_landmarks.squeeze().shape)\n",
        "        output_face_landmarks = output_face_landmarks.reshape(39,5)\n",
        "        output_face_landmarks = output_face_landmarks[:33,:]\n",
        "        face_landmark_x = output_face_landmarks[:, 0:1]/255*image_cropped.shape[1]\n",
        "        face_landmark_y = output_face_landmarks[:, 1:2]/255*image_cropped.shape[0]\n",
        "        z= output_face_landmarks[:, 2:3]\n",
        "        visibilty = output_face_landmarks[:, 3:4]\n",
        "        presence = output_face_landmarks[:, 4:5]\n",
        "        if mask >0.98:        \n",
        "            for i in range(len(face_landmark_x)):\n",
        "                if presence[i]>5:\n",
        "                    image_cropped = cv2.circle(image_cropped, (int((face_landmark_x[i])),int(face_landmark_y[i])), 3, (0,0,255), -1)\n",
        "        cv2.imshow(\"window\",image_cropped)\n",
        "        if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "          break\n",
        "    except:\n",
        "        break\n",
        "  \n",
        "    # Display the resulting frame\n",
        "    \n",
        "  \n",
        "# After the loop release the cap object\n",
        "vid.release()\n",
        "# Destroy all the windows\n",
        "cv2.destroyAllWindows()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "gilr-G4yV4Xv"
      },
      "outputs": [],
      "source": [
        "# get details for each layer\n",
        "all_layers_details = interpreter.get_tensor_details() \n",
        "# for layer in all_layers_details:\n",
        "#   print(layer) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jQ01xvC3Wr9z",
        "outputId": "40df5d4c-958b-4790-946c-cec07c35b457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content\n"
          ]
        }
      ],
      "source": [
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "DIU_mca5V7O2"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "\n",
        "csv_columns = ['name','index','shape', 'shape_signature', 'dtype', 'quantization', 'quantization_parameters', 'sparsity_parameters' ]\n",
        "csv_file = \"Layers.csv\"\n",
        "try:\n",
        "    with open(csv_file, 'w') as csvfile:\n",
        "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
        "        writer.writeheader()\n",
        "        for layer in all_layers_details:\n",
        "            writer.writerow(layer)\n",
        "except IOError:\n",
        "    print(\"I/O error\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNTHTrRUyMxVpaJIbFqOxRp",
      "include_colab_link": true,
      "name": "Inspect_layer.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "fbc768028c3e6ead51d9a200ddcb2ec858ae62844dcd1994729a8279be9b48f2"
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
